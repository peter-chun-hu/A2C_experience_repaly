{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from arguments import get_args\n",
    "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from baselines.common.vec_env.vec_normalize import VecNormalize\n",
    "#from envs import make_env\n",
    "from model import Policy\n",
    "from storage_experience_replay import RolloutStorage\n",
    "#from visualize import visdom_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import algo\n",
    "\n",
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym.spaces.box import Box\n",
    "\n",
    "from baselines import bench\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '/datasets/home/54/454/ee276cbc/.local/lib/python3.6/site-packages', '/datasets/home/54/454/ee276cbc/baselines', '/opt/conda/lib/python3.6/site-packages', '/opt/conda/lib/python3.6/site-packages/Mako-1.0.7-py3.6.egg', '/baselines', '/mujoco-py', '/opt/conda/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg', '/opt/conda/lib/python3.6/site-packages/IPython/extensions', '/datasets/home/54/454/ee276cbc/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_timestep = False\n",
    "algorithm = 'a2c'\n",
    "alpha = 0.99\n",
    "clip_param=0.2\n",
    "cuda = True\n",
    "entropy_coef=0.01\n",
    "\n",
    "env_name = \"PongNoFrameskip-v4\"\n",
    "#env_name  = \"BreakoutNoFrameskip-v4\"\n",
    "#env_name  = 'CartPole-v0'\n",
    "#env_name = 'Pendulum-v0'\n",
    "#env_name = \"MountainCar-v0\"\n",
    "#env_name = 'InvertedPendulum-v2'\n",
    "#env_name = \"Acrobot-v1\"\n",
    "#env_name = 'HalfCheetah-v2' \n",
    "#env_name = \"DoubleDunkNoFrameskip-v0\"\n",
    "weight_decay=0\n",
    "eps=1e-05\n",
    "gamma=0.99\n",
    "\n",
    "log_dir='gym' +'/' + env_name\n",
    "log_interval = 10\n",
    "lr = 0.0007\n",
    "\n",
    "max_grad_norm=0.5\n",
    "no_cuda=False\n",
    "no_vis=False\n",
    "num_frames=10000000.0\n",
    "num_mini_batch=128\n",
    "num_processes=1\n",
    "num_stack=4\n",
    "num_steps=200\n",
    "port=8097\n",
    "ppo_epoch=4\n",
    "recurrent_policy=False\n",
    "save_dir='trained_models'\n",
    "save_interval=1000\n",
    "seed=1\n",
    "tau=0.95\n",
    "use_gae=False\n",
    "value_loss_coef=0.5\n",
    "vis=True\n",
    "vis_interval=100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddTimestep(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(AddTimestep, self).__init__(env)\n",
    "        self.observation_space = Box(\n",
    "            self.observation_space.low[0],\n",
    "            self.observation_space.high[0],\n",
    "            [self.observation_space.shape[0] + 1],\n",
    "            dtype=self.observation_space.dtype)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.concatenate((observation, [self.env._elapsed_steps]))\n",
    "\n",
    "\n",
    "class WrapPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(WrapPyTorch, self).__init__(env)\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = Box(\n",
    "            self.observation_space.low[0, 0, 0],\n",
    "            self.observation_space.high[0, 0, 0],\n",
    "            [obs_shape[2], obs_shape[1], obs_shape[0]],\n",
    "            dtype=self.observation_space.dtype)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return observation.transpose(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed, rank, log_dir, add_timestep):\n",
    "    \n",
    "    def _thunk():\n",
    "        if env_id.startswith(\"dm\"):\n",
    "            _, domain, task = env_id.split('.')\n",
    "            env = dm_control2gym.make(domain_name=domain, task_name=task)\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        \n",
    "        \n",
    "        is_atari = hasattr(gym.envs, 'atari') and isinstance(\n",
    "            env.unwrapped, gym.envs.atari.atari_env.AtariEnv)\n",
    "        if is_atari:\n",
    "            env = make_atari(env_id)\n",
    "        env.seed(seed + rank)\n",
    "\n",
    "        obs_shape = env.observation_space.shape\n",
    "        if add_timestep and len(\n",
    "                obs_shape) == 1 and str(env).find('TimeLimit') > -1:\n",
    "            env = AddTimestep(env)\n",
    "        '''  \n",
    "        if log_dir is not None:\n",
    "            env = bench.Monitor(env, os.path.join(log_dir, str(rank)))\n",
    "        ''' \n",
    "        logging_interval = 50\n",
    "        VISUALIZE = True\n",
    "        if not os.path.exists('video'):\n",
    "            os.makedirs('video')\n",
    "        \n",
    "        if VISUALIZE:\n",
    "            if not os.path.exists('video' + '/' + env_id):\n",
    "                os.mkdir('video' + '/' + env_id)\n",
    "            env = gym.wrappers.Monitor(env, 'video' + '/' + env_id, force=True, video_callable=lambda episode_id: episode_id%logging_interval==0)\n",
    "          \n",
    "        #env.reset()\n",
    "        #env.render()    \n",
    "            \n",
    "            \n",
    "        if is_atari:\n",
    "            env = wrap_deepmind(env)\n",
    "\n",
    "        # If the input has shape (W,H,3), wrap for PyTorch convolutions\n",
    "        obs_shape = env.observation_space.shape\n",
    "        if len(obs_shape) == 3 and obs_shape[2] in [1, 3]:\n",
    "            env = WrapPyTorch(env)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return _thunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('npy'):\n",
    "    os.makedirs('npy')\n",
    "if not os.path.exists('plot'):\n",
    "    os.makedirs('plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert algorithm in ['a2c', 'ppo', 'acktr']\n",
    "if recurrent_policy:\n",
    "    assert algorithm in ['a2c', 'ppo'], \\\n",
    "        'Recurrent policy is not implemented for ACKTR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "num_updates = int(num_frames) // num_steps // num_processes\n",
    "print(num_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(log_dir, '*.monitor.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######\n",
      "WARNING: All rewards are clipped or normalized so you need to use a monitor (see envs.py) or visdom plot to get true rewards\n",
      "#######\n"
     ]
    }
   ],
   "source": [
    "print(\"#######\")\n",
    "print(\"WARNING: All rewards are clipped or normalized so you need to use a monitor (see envs.py) or visdom plot to get true rewards\")\n",
    "print(\"#######\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.set_num_threads(1)\\nif vis:\\n    from visdom import Visdom\\n    viz = Visdom(port=port)\\n    win = None\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.set_num_threads(1)\n",
    "if vis:\n",
    "    from visdom import Visdom\n",
    "    viz = Visdom(port=port)\n",
    "    win = None\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<function make_env.<locals>._thunk at 0x7f343147d1e0>\n"
     ]
    }
   ],
   "source": [
    "print(num_processes)\n",
    "envs = [make_env(env_name, seed, i, log_dir, add_timestep)\n",
    "            for i in range(num_processes)]\n",
    "print(envs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_processes > 1:\n",
    "    envs = SubprocVecEnv(envs)\n",
    "else:\n",
    "    envs = DummyVecEnv(envs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(envs.observation_space.shape) == 1:\n",
    "        envs = VecNormalize(envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_shape = envs.observation_space.shape\n",
    "obs_shape = (obs_shape[0] * num_stack, *obs_shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic = Policy(obs_shape, envs.action_space, recurrent_policy)\n",
    "#save_path = os.path.join(save_dir, algorithm)\n",
    "#saved_model = torch.load(os.path.join(save_path, env_name + \".pt\"))\n",
    "#actor_critic.load_state_dict(saved_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if envs.action_space.__class__.__name__ == \"Discrete\":\n",
    "    action_shape = 1\n",
    "else:\n",
    "    action_shape = envs.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    actor_critic.cuda()\n",
    "\n",
    "\n",
    "if algorithm == 'a2c':\n",
    "    agent = algo.A2C_ACKTR(actor_critic, value_loss_coef,\n",
    "                           entropy_coef, lr=lr,\n",
    "                           eps=eps, alpha=alpha,\n",
    "                           max_grad_norm=max_grad_norm, weight_decay=weight_decay)\n",
    "elif algorithm == 'ppo':\n",
    "    agent = algo.PPO(actor_critic, clip_param, ppo_epoch, num_mini_batch,\n",
    "                     value_loss_coef, entropy_coef, lr=lr,\n",
    "                           eps=eps,\n",
    "                           max_grad_norm=max_grad_norm)\n",
    "elif algorithm == 'acktr':\n",
    "    agent = algo.A2C_ACKTR(actor_critic, value_loss_coef,\n",
    "                           entropy_coef, acktr=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts = RolloutStorage(buffer_size=100, num_steps =num_steps, num_processes=num_processes, \\\n",
    "                          obs_shape=obs_shape, action_space=envs.action_space, state_size=actor_critic.state_size)\n",
    "current_obs = torch.zeros(num_processes, *obs_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_current_obs(obs):\n",
    "    shape_dim0 = envs.observation_space.shape[0]\n",
    "    obs = torch.from_numpy(obs).float()\n",
    "    if num_stack > 1:\n",
    "        current_obs[:, :-shape_dim0] = current_obs[:, shape_dim0:]\n",
    "    current_obs[:, -shape_dim0:] = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = envs.reset()\n",
    "envs.render()\n",
    "update_current_obs(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts.observations[0,0].copy_(current_obs)\n",
    "print(rollouts.observations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables are used to compute average rewards for all processes.\n",
    "episode_rewards = torch.zeros([num_processes, 1])\n",
    "final_rewards = torch.zeros([num_processes, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    current_obs = current_obs.cuda()\n",
    "    rollouts.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "reward_mean_arr = []\n",
    "\n",
    "for j in range(num_updates):\n",
    "    \n",
    "    memory_size = rollouts.memory_size\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        if j % save_interval == 0 :\n",
    "            envs.render()\n",
    "            \n",
    "            time.sleep(0.05)\n",
    "        \n",
    "        \n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, states = actor_critic.act(\n",
    "                    rollouts.observations[memory_size,step],\n",
    "                    rollouts.states[memory_size,step],\n",
    "                    rollouts.masks[memory_size,step])\n",
    "            \n",
    "        cpu_actions = action.squeeze(1).cpu().numpy()\n",
    "        #print(np.asarray(cpu_actions))\n",
    "        # Obser reward and next obs\n",
    "        \n",
    "        if env_name == 'Pendulum-v0':\n",
    "            obs, reward, done, info = envs.step(np.expand_dims(np.asarray(cpu_actions), axis=0) )\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            obs, reward, done, info = envs.step(cpu_actions)\n",
    "            \n",
    "        reward = torch.from_numpy(np.expand_dims(np.stack(reward), 1)).float()\n",
    "        episode_rewards += reward\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "        final_rewards *= masks\n",
    "        final_rewards += (1 - masks) * episode_rewards\n",
    "        episode_rewards *= masks\n",
    "\n",
    "        if cuda:\n",
    "            masks = masks.cuda()\n",
    "\n",
    "        if current_obs.dim() == 4:\n",
    "            current_obs *= masks.unsqueeze(2).unsqueeze(2)\n",
    "        else:\n",
    "            current_obs *= masks\n",
    "\n",
    "        update_current_obs(obs)\n",
    "        rollouts.insert(current_obs, states, action, action_log_prob, value, reward, masks)\n",
    "        \n",
    "        \n",
    "        if done and  env_name == 'Pendulum-v0':\n",
    "            break\n",
    "        \n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_value = actor_critic.get_value(rollouts.observations[memory_size, -1],\n",
    "                                            rollouts.states[memory_size, -1],\n",
    "                                            rollouts.masks[memory_size, -1]).detach()\n",
    "\n",
    "    rollouts.compute_returns(next_value, use_gae, gamma, tau)\n",
    "\n",
    "    #value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "    rollouts_batch = rollouts.sample(50)\n",
    "    value_loss, action_loss, dist_entropy = agent.update_experience_replay(rollouts_batch)\n",
    "\n",
    "    rollouts.insert_new_trajectory()\n",
    "    \n",
    "\n",
    "    if j % 1 == 0 and save_dir != \"\":\n",
    "        save_path = os.path.join(save_dir, algorithm)\n",
    "        try:\n",
    "            os.makedirs(save_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        # A really ugly way to save a model to CPU\n",
    "        save_model = actor_critic\n",
    "        if cuda:\n",
    "            save_model = copy.deepcopy(actor_critic).cpu()\n",
    "\n",
    "        #save_model = [save_model,\n",
    "        #                hasattr(envs, 'ob_rms') and envs.ob_rms or None]\n",
    "\n",
    "        torch.save(save_model, os.path.join(save_path, env_name + \".pt\"))\n",
    "\n",
    "    if j % log_interval == 0:\n",
    "        end = time.time()\n",
    "        total_num_steps = (j + 1) * num_processes * num_steps\n",
    "        print(\"Updates {}, num timesteps {}, FPS {}, mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}, entropy {:.5f}, value loss {:.5f}, policy loss {:.5f}\".\n",
    "            format(j, total_num_steps,\n",
    "                   int(total_num_steps / (end - start)),\n",
    "                   final_rewards.mean(),\n",
    "                   final_rewards.median(),\n",
    "                   final_rewards.min(),\n",
    "                   final_rewards.max(), dist_entropy,\n",
    "                   value_loss, action_loss))\n",
    "        \n",
    "        reward_mean_arr.append(np.asscalar(final_rewards.mean().cpu().data.numpy()))\n",
    "    \n",
    "    #print(final_rewards.shape)\n",
    "    \n",
    "    if vis and j % save_interval == 0:\n",
    "        \n",
    "        np.save('npy/reward_mean_' + str(j)+ '.npy' , np.asarray(reward_mean_arr))\n",
    "        plt.plot(reward_mean_arr)\n",
    "        plt.savefig('plot/reward_mean_' + str(j)+ '.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
