{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from arguments import get_args\n",
    "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from baselines.common.vec_env.vec_normalize import VecNormalize\n",
    "#from envs import make_env\n",
    "from model import Policy\n",
    "from storage import RolloutStorage\n",
    "#from visualize import visdom_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import algo\n",
    "\n",
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym.spaces.box import Box\n",
    "\n",
    "from baselines import bench\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_timestep = False\n",
    "algorithm = 'a2c'\n",
    "alpha = 0.99\n",
    "clip_param=0.2\n",
    "cuda = True\n",
    "entropy_coef=0.01\n",
    "#env_name = \"PongNoFrameskip-v4\"\n",
    "env_name  = \"BreakoutNoFrameskip-v4\"\n",
    "eps=1e-05\n",
    "gamma=0.99\n",
    "log_dir='gym'\n",
    "log_interval = 10\n",
    "lr = 0.0007\n",
    "max_grad_norm=0.5\n",
    "no_cuda=False\n",
    "no_vis=False\n",
    "num_frames=10000000.0\n",
    "num_mini_batch=32\n",
    "num_processes=1\n",
    "num_stack=4\n",
    "num_steps=5\n",
    "port=8097\n",
    "ppo_epoch=4\n",
    "recurrent_policy=False\n",
    "save_dir='trained_models'\n",
    "save_interval=1000\n",
    "seed=1\n",
    "tau=0.95\n",
    "use_gae=False\n",
    "value_loss_coef=0.5\n",
    "vis=True\n",
    "vis_interval=100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddTimestep(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(AddTimestep, self).__init__(env)\n",
    "        self.observation_space = Box(\n",
    "            self.observation_space.low[0],\n",
    "            self.observation_space.high[0],\n",
    "            [self.observation_space.shape[0] + 1],\n",
    "            dtype=self.observation_space.dtype)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.concatenate((observation, [self.env._elapsed_steps]))\n",
    "\n",
    "\n",
    "class WrapPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(WrapPyTorch, self).__init__(env)\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = Box(\n",
    "            self.observation_space.low[0, 0, 0],\n",
    "            self.observation_space.high[0, 0, 0],\n",
    "            [obs_shape[2], obs_shape[1], obs_shape[0]],\n",
    "            dtype=self.observation_space.dtype)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return observation.transpose(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed, rank, log_dir, add_timestep):\n",
    "    \n",
    "    def _thunk():\n",
    "        if env_id.startswith(\"dm\"):\n",
    "            _, domain, task = env_id.split('.')\n",
    "            env = dm_control2gym.make(domain_name=domain, task_name=task)\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        \n",
    "        \n",
    "        is_atari = hasattr(gym.envs, 'atari') and isinstance(\n",
    "            env.unwrapped, gym.envs.atari.atari_env.AtariEnv)\n",
    "        if is_atari:\n",
    "            env = make_atari(env_id)\n",
    "        env.seed(seed + rank)\n",
    "\n",
    "        obs_shape = env.observation_space.shape\n",
    "        if add_timestep and len(\n",
    "                obs_shape) == 1 and str(env).find('TimeLimit') > -1:\n",
    "            env = AddTimestep(env)\n",
    "\n",
    "        #if log_dir is not None:\n",
    "        #    env = bench.Monitor(env, os.path.join(log_dir, str(rank)))\n",
    "\n",
    "        logging_interval = 50\n",
    "        VISUALIZE = True\n",
    "        if not os.path.exists('video'):\n",
    "            os.makedirs('video')\n",
    "        if VISUALIZE:\n",
    "            if not os.path.exists('video' + '/' + env_id):\n",
    "                os.mkdir('video' + '/' + env_id)\n",
    "            env = gym.wrappers.Monitor(env, 'video' + '/' + env_id, force=True, video_callable=lambda episode_id: episode_id%logging_interval==0)\n",
    "\n",
    "        #env.reset()\n",
    "        #env.render()    \n",
    "            \n",
    "            \n",
    "        if is_atari:\n",
    "            env = wrap_deepmind(env)\n",
    "\n",
    "        # If the input has shape (W,H,3), wrap for PyTorch convolutions\n",
    "        obs_shape = env.observation_space.shape\n",
    "        if len(obs_shape) == 3 and obs_shape[2] in [1, 3]:\n",
    "            env = WrapPyTorch(env)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return _thunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('npy'):\n",
    "    os.makedirs('npy')\n",
    "if not os.path.exists('plot'):\n",
    "    os.makedirs('plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert algorithm in ['a2c', 'ppo', 'acktr']\n",
    "if recurrent_policy:\n",
    "    assert algorithm in ['a2c', 'ppo'], \\\n",
    "        'Recurrent policy is not implemented for ACKTR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n"
     ]
    }
   ],
   "source": [
    "num_updates = int(num_frames) // num_steps // num_processes\n",
    "print(num_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(log_dir, '*.monitor.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######\n",
      "WARNING: All rewards are clipped or normalized so you need to use a monitor (see envs.py) or visdom plot to get true rewards\n",
      "#######\n"
     ]
    }
   ],
   "source": [
    "print(\"#######\")\n",
    "print(\"WARNING: All rewards are clipped or normalized so you need to use a monitor (see envs.py) or visdom plot to get true rewards\")\n",
    "print(\"#######\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.set_num_threads(1)\\nif vis:\\n    from visdom import Visdom\\n    viz = Visdom(port=port)\\n    win = None\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.set_num_threads(1)\n",
    "if vis:\n",
    "    from visdom import Visdom\n",
    "    viz = Visdom(port=port)\n",
    "    win = None\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<function make_env.<locals>._thunk at 0x7f288638a6a8>\n"
     ]
    }
   ],
   "source": [
    "print(num_processes)\n",
    "envs = [make_env(env_name, seed, i, log_dir, add_timestep)\n",
    "            for i in range(num_processes)]\n",
    "print(envs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_processes > 1:\n",
    "    envs = SubprocVecEnv(envs)\n",
    "else:\n",
    "    envs = DummyVecEnv(envs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(envs.observation_space.shape) == 1:\n",
    "        envs = VecNormalize(envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_shape = envs.observation_space.shape\n",
    "obs_shape = (obs_shape[0] * num_stack, *obs_shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic = Policy(obs_shape, envs.action_space, recurrent_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if envs.action_space.__class__.__name__ == \"Discrete\":\n",
    "    action_shape = 1\n",
    "else:\n",
    "    action_shape = envs.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    actor_critic.cuda()\n",
    "\n",
    "\n",
    "if algorithm == 'a2c':\n",
    "    agent = algo.A2C_ACKTR(actor_critic, value_loss_coef,\n",
    "                           entropy_coef, lr=lr,\n",
    "                           eps=eps, alpha=alpha,\n",
    "                           max_grad_norm=max_grad_norm)\n",
    "elif algorithm == 'ppo':\n",
    "    agent = algo.PPO(actor_critic, clip_param, ppo_epoch, num_mini_batch,\n",
    "                     value_loss_coef, entropy_coef, lr=lr,\n",
    "                           eps=eps,\n",
    "                           max_grad_norm=max_grad_norm)\n",
    "elif algorithm == 'acktr':\n",
    "    agent = algo.A2C_ACKTR(actor_critic, value_loss_coef,\n",
    "                           entropy_coef, acktr=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts = RolloutStorage(num_steps, num_processes, obs_shape, envs.action_space, actor_critic.state_size)\n",
    "current_obs = torch.zeros(num_processes, *obs_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_current_obs(obs):\n",
    "    shape_dim0 = envs.observation_space.shape[0]\n",
    "    obs = torch.from_numpy(obs).float()\n",
    "    if num_stack > 1:\n",
    "        current_obs[:, :-shape_dim0] = current_obs[:, shape_dim0:]\n",
    "    current_obs[:, -shape_dim0:] = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          ...,\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
       "\n",
       "         [[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          ...,\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
       "\n",
       "         [[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          ...,\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.]],\n",
       "\n",
       "         [[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          ...,\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
       "          [   0.,    0.,    0.,  ...,    0.,    0.,    0.]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = envs.reset()\n",
    "envs.render()\n",
    "update_current_obs(obs)\n",
    "rollouts.observations[0].copy_(current_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables are used to compute average rewards for all processes.\n",
    "episode_rewards = torch.zeros([num_processes, 1])\n",
    "final_rewards = torch.zeros([num_processes, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    current_obs = current_obs.cuda()\n",
    "    rollouts.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 0, num timesteps 5, FPS 14, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.38501, value loss 0.00001, policy loss -0.00387\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 10, num timesteps 55, FPS 69, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.38499, value loss 0.00149, policy loss -0.02411\n",
      "Updates 20, num timesteps 105, FPS 103, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.38533, value loss 0.00002, policy loss -0.00552\n",
      "Updates 30, num timesteps 155, FPS 124, mean/median reward 1.0/1.0, min/max reward 1.0/1.0, entropy 1.38536, value loss 0.58316, policy loss 0.79755\n",
      "Updates 40, num timesteps 205, FPS 138, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.38481, value loss 0.00001, policy loss -0.00406\n",
      "Updates 50, num timesteps 255, FPS 153, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.38409, value loss 0.00003, policy loss -0.00679\n",
      "Updates 60, num timesteps 305, FPS 164, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.38424, value loss 0.00002, policy loss -0.00573\n",
      "Updates 70, num timesteps 355, FPS 172, mean/median reward 1.0/1.0, min/max reward 1.0/1.0, entropy 1.38394, value loss 0.00003, policy loss -0.00660\n",
      "Updates 80, num timesteps 405, FPS 180, mean/median reward 1.0/1.0, min/max reward 1.0/1.0, entropy 1.38459, value loss 0.00003, policy loss -0.00661\n",
      "Updates 90, num timesteps 455, FPS 183, mean/median reward 1.0/1.0, min/max reward 1.0/1.0, entropy 1.38491, value loss 0.00004, policy loss -0.00801\n",
      "Updates 100, num timesteps 505, FPS 188, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.38506, value loss 0.01834, policy loss -0.18582\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-588f44d1e43d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_returns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ece276/final/my_own/276_hw4/algo/a2c_acktr.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         (value_loss * self.value_loss_coef + action_loss -\n\u001b[0;32m---> 71\u001b[0;31m          dist_entropy * self.entropy_coef).backward()\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macktr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "reward_mean_arr = []\n",
    "\n",
    "for j in range(num_updates):\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        if j % save_interval == 0 :\n",
    "            envs.render()\n",
    "            time.sleep(0.05)\n",
    "        \n",
    "        \n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, states = actor_critic.act(\n",
    "                    rollouts.observations[step],\n",
    "                    rollouts.states[step],\n",
    "                    rollouts.masks[step])\n",
    "        cpu_actions = action.squeeze(1).cpu().numpy()\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, reward, done, info = envs.step(cpu_actions)\n",
    "        reward = torch.from_numpy(np.expand_dims(np.stack(reward), 1)).float()\n",
    "        episode_rewards += reward\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "        final_rewards *= masks\n",
    "        final_rewards += (1 - masks) * episode_rewards\n",
    "        episode_rewards *= masks\n",
    "\n",
    "        if cuda:\n",
    "            masks = masks.cuda()\n",
    "\n",
    "        if current_obs.dim() == 4:\n",
    "            current_obs *= masks.unsqueeze(2).unsqueeze(2)\n",
    "        else:\n",
    "            current_obs *= masks\n",
    "\n",
    "        update_current_obs(obs)\n",
    "        rollouts.insert(current_obs, states, action, action_log_prob, value, reward, masks)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_value = actor_critic.get_value(rollouts.observations[-1],\n",
    "                                            rollouts.states[-1],\n",
    "                                            rollouts.masks[-1]).detach()\n",
    "\n",
    "    rollouts.compute_returns(next_value, use_gae, gamma, tau)\n",
    "\n",
    "    value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "\n",
    "    rollouts.after_update()\n",
    "\n",
    "    if j % save_interval == 0 and save_dir != \"\":\n",
    "        save_path = os.path.join(save_dir, algorithm)\n",
    "        try:\n",
    "            os.makedirs(save_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        # A really ugly way to save a model to CPU\n",
    "        save_model = actor_critic\n",
    "        if cuda:\n",
    "            save_model = copy.deepcopy(actor_critic).cpu()\n",
    "\n",
    "        #save_model = [save_model,\n",
    "        #                hasattr(envs, 'ob_rms') and envs.ob_rms or None]\n",
    "\n",
    "        torch.save(save_model, os.path.join(save_path, env_name + \".pt\"))\n",
    "\n",
    "    if j % log_interval == 0:\n",
    "        end = time.time()\n",
    "        total_num_steps = (j + 1) * num_processes * num_steps\n",
    "        print(\"Updates {}, num timesteps {}, FPS {}, mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}, entropy {:.5f}, value loss {:.5f}, policy loss {:.5f}\".\n",
    "            format(j, total_num_steps,\n",
    "                   int(total_num_steps / (end - start)),\n",
    "                   final_rewards.mean(),\n",
    "                   final_rewards.median(),\n",
    "                   final_rewards.min(),\n",
    "                   final_rewards.max(), dist_entropy,\n",
    "                   value_loss, action_loss))\n",
    "        \n",
    "        reward_mean_arr.append(np.asscalar(final_rewards.mean().cpu().data.numpy()))\n",
    "    \n",
    "    #print(final_rewards.shape)\n",
    "    \n",
    "    if vis and j % save_interval == 0:\n",
    "        \n",
    "        np.save('npy/reward_mean_' + str(j)+ '.npy' , np.asarray(reward_mean_arr))\n",
    "        plt.plot(reward_mean_arr)\n",
    "        plt.savefig('plot/reward_mean_' + str(j)+ '.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
